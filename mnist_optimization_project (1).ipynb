{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0fa59f-7f43-4140-ba46-8cc66ee70c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f904e-f6a3-4027-917f-55f58088fbd7",
   "metadata": {},
   "source": [
    "Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731c5904-417e-41bc-abd9-0d97c920243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(binary_classification=True):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    if binary_classification:\n",
    "        train = [(x, float(y == 0)) for x, y in train]\n",
    "        test = [(x, float(y == 0)) for x, y in test]\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5dac3-686f-405e-b391-37b8b2f086a6",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2412aaaf-c8f4-445a-9532-afc381fdd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.w = np.zeros((input_dim, 1))\n",
    "        self.b = 0\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(X @ self.w + self.b)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        m = len(y)\n",
    "        preds = self.predict(X)\n",
    "        loss = -np.mean(y * np.log(preds + 1e-8) + (1 - y) * np.log(1 - preds + 1e-8))\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        preds = self.predict(X)\n",
    "        error = preds - y\n",
    "        dw = (X.T @ error) / m\n",
    "        db = np.mean(error)\n",
    "        return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea185f5-1918-4c62-9861-fb0c0a4fe9ad",
   "metadata": {},
   "source": [
    " SGD Optimizer (Student: extend this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84e3099-022e-403e-b7f9-427eb212e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_train(model, train_loader, lr=0.01, num_epochs=10):\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            X = x_batch.numpy()\n",
    "            y = y_batch.numpy().reshape(-1, 1)\n",
    "\n",
    "            dw, db = model.gradient(X, y)\n",
    "            model.w -= lr * dw\n",
    "            model.b -= lr * db\n",
    "\n",
    "            loss = model.loss(X, y)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662264c",
   "metadata": {},
   "source": [
    "SGD-Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b240405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum_train(model, train_loader, lr=0.01, num_epochs=10, beta=0.9):\n",
    "    loss_history = []\n",
    "    v_w = np.zeros_like(model.w)\n",
    "    v_b = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            X = x_batch.numpy()\n",
    "            y = y_batch.numpy().reshape(-1, 1)\n",
    "\n",
    "            dw, db = model.gradient(X, y)\n",
    "\n",
    "            # Update velocities\n",
    "            v_w = beta * v_w + (1 - beta) * dw\n",
    "            v_b = beta * v_b + (1 - beta) * db\n",
    "\n",
    "            # Update parameters\n",
    "            model.w -= lr * v_w\n",
    "            model.b -= lr * v_b\n",
    "\n",
    "            loss = model.loss(X, y)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b58a2",
   "metadata": {},
   "source": [
    "SGD-Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374c519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_adam_train(model, train_loader, lr=0.01, num_epochs=10, beta1=0.9, beta2=0.999, epsilon=1e-8, bias_correction=True):\n",
    "    loss_history = []\n",
    "    m_w = np.zeros_like(model.w)\n",
    "    v_w = np.zeros_like(model.w)\n",
    "    m_b = 0\n",
    "    v_b = 0\n",
    "    t = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            X = x_batch.numpy()\n",
    "            y = y_batch.numpy().reshape(-1, 1)\n",
    "\n",
    "            dw, db = model.gradient(X, y)\n",
    "\n",
    "            # Increment timestep\n",
    "            t += 1\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            m_w = beta1 * m_w + (1 - beta1) * dw\n",
    "            m_b = beta1 * m_b + (1 - beta1) * db\n",
    "\n",
    "            # Update biased second raw moment estimate\n",
    "            v_w = beta2 * v_w + (1 - beta2) * (dw ** 2)\n",
    "            v_b = beta2 * v_b + (1 - beta2) * (db ** 2)\n",
    "\n",
    "            # Bias correction\n",
    "            if bias_correction:\n",
    "                m_w_hat = m_w / (1 - beta1 ** t)\n",
    "                m_b_hat = m_b / (1 - beta1 ** t)\n",
    "                v_w_hat = v_w / (1 - beta2 ** t)\n",
    "                v_b_hat = v_b / (1 - beta2 ** t)\n",
    "            else:\n",
    "            # No bias correction\n",
    "                m_w_hat = m_w\n",
    "                m_b_hat = m_b\n",
    "                v_w_hat = v_w\n",
    "                v_b_hat = v_b\n",
    "\n",
    "            # Update parameters\n",
    "            model.w -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "            model.b -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "            loss = model.loss(X, y)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8646faf-7c72-4a82-b993-7cdac09e30bf",
   "metadata": {},
   "source": [
    "Plotting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63d9cb8-ed35-41b6-a877-790876a68830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, label='SGD'):\n",
    "    plt.plot(losses, label=label)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Iteration\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6c826-c0d6-4ccd-ad05-dc329d77f04c",
   "metadata": {},
   "source": [
    " Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed73145c-b25c-4edb-a84b-379614c55178",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2257804266.py, line 28)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplt.title(\"Loss Comparison Across Optimizers\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_loader, test_loader = load_mnist()\n",
    "    input_dim = 28 * 28\n",
    "\n",
    "    # Vanilla SGD\n",
    "    model_sgd = LogisticRegressionModel(input_dim=input_dim)\n",
    "    loss_sgd = sgd_train(model_sgd, train_loader, lr=0.1, num_epochs=5)\n",
    "    plot_loss(loss_sgd, label='SGD (lr=0.1)')\n",
    "\n",
    "    # SGD + Momentum\n",
    "    model_momentum = LogisticRegressionModel(input_dim=input_dim)\n",
    "    loss_momentum = sgd_momentum_train(model_momentum, train_loader, lr=0.1, num_epochs=5, beta=0.9)\n",
    "    plot_loss(loss_momentum, label='Momentum (Î²=0.9)')\n",
    "\n",
    "# Adam Optimizer\n",
    "# With bias correction\n",
    "model_adam_corrected = LogisticRegressionModel(input_dim=input_dim)\n",
    "loss_adam_corrected = sgd_adam_train(model_adam_corrected, train_loader, lr=0.001, num_epochs=5, beta1=0.9, beta2=0.99, epsilon=1e-8, bias_correction=True)\n",
    "plot_loss(loss_adam_corrected, label='Adam (Bias Correction)')\n",
    "\n",
    "# Without bias correction\n",
    "model_adam_nocorrect = LogisticRegressionModel(input_dim=input_dim)\n",
    "loss_adam_nocorrect = sgd_adam_train(model_adam_nocorrect, train_loader, lr=0.001, num_epochs=5, beta1=0.9, beta2=0.99, epsilon=1e-8, bias_correction=False)\n",
    "plot_loss(loss_adam_nocorrect, label='Adam (No Bias Correction)')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.title(\"Loss Comparison Across Optimizers\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
